---
title: "Sentiment Code Only"
output:
  html_document: default
  word_document: default
---

In this code: 
- Explore Sentiment through Unigrams and Bigrams 
- Calculate TF-IDF for unigrams/bigrams 
- Calculate sentiment scores with 4 lexicon dictionaries 

#Load packages 

```{r} 
library(dplyr)
library(ggplot2)
library(tm)
library(tidytext)
library(textstem)
library(stringr)
library(wordcloud)
library(tidyr)
```

#Load clean data files from Part 1 (Exploratory Code) 

```{r}
dfclean <- read.csv("dfclean.csv", na.strings="", stringsAsFactors = FALSE)
revclean <- read.csv("reviewsclean.csv", na.strings="")

str(dfclean)
str(revclean)
```

Join the two files together by listing_id 

```{r}
#right outer join by listing_id
mr <- right_join(dfclean, revclean, by = "listing_id")
mr <- subset(mr, select=-c(reviewer_name))

#convert date to date format
mr$date <- as.Date(mr$date, format = "%Y-%m-%d")
mr$last_review <- as.Date(mr$last_review, format ="%Y-%m-%d")

#as.character for comments
mr$comments <- as.character(mr$comments)
head(mr)

#check for missing values 
colSums(is.na(mr))
```

```{r}
#subset listings with 12 or more reviews per month
topRPM <- 
  mr %>%
  filter(reviews_per_month >= 12)
View(topRPM)
```

#TIDY TEXT - UNIGRAMS

##Data Preprocessing + Tokenize
Dataframe to tidy text format 
- tokenize 
- remove non-alphabetic characters (numbers & non-english characters) 
- remove stopwords 
- lemmatize

LEMMATIZATION - takes into consideration the morphological analysis of the words. 
lemmatize_words() from the textstem package 
lemmatize_strings() to lemmatize words within a string without extract the words

By default, unnest_tokens() 
- converts tokens to lowercase 
- strip punctuations
- strip white spaces 


```{r}
#Create custom list of stop words from english 
custom_stop_words <- tibble(word=c("toronto","airbnb","holiday","vacation","highly","recommend", "stay"))

#UNNEST_TOKENS into tidy dataset - uses tokenizers package
review_comments <- mr %>% 
  unnest_tokens(word, comments) %>% #tokenize
  filter(!str_detect(word, "[^[:alpha:]]")) %>%
  anti_join(stop_words, by="word") %>% 
  anti_join(custom_stop_words, by = "word") %>% #remove stopwords 
  mutate(word = lemmatize_words(word))  #lemmatize 

head(review_comments,n=20)
names(review_comments)
```


Count the most popular words 
```{r}
review_comments %>%
  count(word, sort = TRUE)

head(review_comments$word, n=20)
```

##Term Frequency 

Calculate Term frequency
- there is one row in neighbourhood_words df for each word-neighbourhood combination. 

```{r}
neighbourhood_words <- review_comments %>%
  count(neighbourhood, word, sort = TRUE)

#summarize total words by neighbourhood 
total_words <- neighbourhood_words %>%
  group_by(neighbourhood) %>%
  summarise(total=sum(n)) %>% arrange(-total)
head(total_words, n=10)

neighbourhood_words <- left_join(neighbourhood_words, total_words)

#view term frequency per word/per neighbourhood 
head(neighbourhood_words, n=10)
```

```{r}
#zif's law states that the frequency that a word appears is inversely proportional to its rank shown below 
#calculate frequency by rank 

freq_by_rank <- neighbourhood_words %>%
  group_by(neighbourhood) %>%
  mutate(rank = row_number(), #rank of each word within the freq table 
         'term frequency' = n/total)

head(freq_by_rank, n=10)
```

##TF-IDF

Inverse document frequency (idf), decreases the weight for commonly used words and increases the weight for words that are not used very much in a collection of documents
- combined with term frequency to calculate a term's tf-idf (two quantities multipled together), the frequency of a term adjusted for how rarely it is used 
- find important words for the content of each document by decreasing the weight for commonly used words and increasing the weight for words that are not used very much 

```{r}
#calculate TF-IDF
neighbourhood_words <- neighbourhood_words %>%
  bind_tf_idf(word, neighbourhood, n)

#view tf-idf dataframe
head(neighbourhood_words, n=10)
```

```{r, echo=FALSE}
#view highest tf-idf total by neighbourhood 
neighbourhood_words %>%
  select(-total) %>%
  arrange(desc(tf_idf))

#subset top 5 tfidf neighbourhoods 
top5neigh_tfidf <- subset(neighbourhood_words, neighbourhood == 'Waterfront Communities-The Island'|neighbourhood == 'Niagara'| neighbourhood =='Annex'| neighbourhood == 'Bay Street Corridor'| neighbourhood == 'Church-Yonge Corridor')

#Facet graph of top 5 neighbourhoods with their highest tf-idf words 
top5neigh_tfidf %>%
  arrange(desc(tf_idf)) %>%
  mutate(word = factor(word, levels = rev(unique(word)))) %>%
  group_by(neighbourhood) %>%
  top_n(15) %>%
  ungroup() %>%
  ggplot(aes(word, tf_idf, fill=neighbourhood)) + 
  geom_col(show.legend=FALSE) + 
  labs(x=NULL, y="tf-idf") + 
  facet_wrap(~neighbourhood, ncol=2, scales="free", shrink = TRUE) + 
  coord_flip()
```
Significant:: 
- waterfront communities highest tf-idf words are cn, tower, condo, view, rogers, arena, scotiabank, acquarium.... shows importance of surrounding neighbourhood 
- the rest of the neighbourhoods highest tf-idf words are mostly names 

View comments with highest tf-idf words 
```{r}
#Raoul - Bay Street Cooridor
mr %>%
  filter(str_detect(comments, "Raoul")) %>%
  select(comments) -> Raoulcomments

Raoulcomments

Raoulcomments#Keir - Church Yonge Cooridor 
mr %>%
  filter(str_detect(comments, "Keir")) %>%
  select(comments) -> Keircomments
```

- The above shows both Raoul and Keir are superhosts.

What if we try removing some names? 

```{r}
mystopwords <- tibble(word=c("toronto",
                             "airbnb",
                             "holiday",
                             "vacation",
                             "highly",
                             "recommend",
                             "stay",
                             "todd",
                             "spencer",
                             "natasha",
                             "frank",
                             "maureen",
                             "irina",
                             "jinty",
                             "jane",
                             "chris",
                             "raoul",
                             "james",
                             "kyla",
                             "charmaine",
                             "connie",
                             "joan",
                             "robert",
                             "matt",
                             "arjun"))

#remove mystopwords from comments
tidy_comments <- anti_join(review_comments, mystopwords, by="word")

#new tfidf 
tidy_tfidf <- tidy_comments %>% 
   count(neighbourhood, word, sort=TRUE) #sort by word 

#calculate term frequency by neighbourhood 
total_tidytfidf <- tidy_tfidf %>%
  group_by(neighbourhood) %>%
  summarise(total=sum(n))

#join 
total_tidytfidf <- left_join(tidy_tfidf, total_tidytfidf)

#calculate tf-idf 
total_tidytfidf <- total_tidytfidf %>%
  bind_tf_idf(word, neighbourhood, n) 

head(total_tidytfidf, n=10)

#top5 neighbourhoods 
#subset top 5 tfidf neighbourhoods 
top5neigh_tfidf2 <- subset(total_tidytfidf, neighbourhood == 'Waterfront Communities-The Island'|neighbourhood == 'Niagara'| neighbourhood =='Annex'| neighbourhood == 'Bay Street Corridor'| neighbourhood == 'Church-Yonge Corridor')

#View facet plot of top 5 neighbourhood with their tf-idf after removing names 
top5neigh_tfidf2 %>%
  arrange(desc(tf_idf)) %>%
  mutate(word = factor(word, levels = rev(unique(word)))) %>%
  group_by(neighbourhood) %>%
  top_n(15) %>%
  ungroup() %>%
  ggplot(aes(word, tf_idf, fill=neighbourhood)) + 
  geom_col(show.legend=FALSE) + 
  labs(x=NULL, y="tf-idf") + 
  facet_wrap(~neighbourhood, ncol=2, scales="free", shrink = TRUE) + 
  coord_flip()
```

Still getting many names as adding top td-idf names onto stopwords only allows other names to come up onto the top 10 tf-idf, however, names can be significant. because they identify superhosts. Thus, can look into what type of comments these superhosts are receiving and why they are as popular as they are.  

```{r, echo=FALSE}
#PLOT MOST COMMON WORDS - unigram
tidy_comments %>% 
  count(word, sort = TRUE) %>%
  filter(n > 50000) %>%
  mutate(word = reorder(word,n)) %>%
  ggplot(aes(word, n)) + 
  geom_col(fill="blue") + 
  xlab("Word") +
  ylab("Frequency") + 
  coord_flip()
```

```{r, echo=FALSE}
#Wordcloud of unigrams
tidy_comments %>% 
  count(word) %>% 
  with(wordcloud(word, n, max.words=80, colors = brewer.pal(8,"Dark2")))
```


##TIDY TEXT Bi-GRAMS - Full dataset 

```{r}
#tokenize bigrams using unnest_tokens
bigrams <- mr %>%
  unnest_tokens(bigram, comments, token = "ngrams", n=2)

#count and filter ngrams
bigrams %>%
  count(bigram, sort=TRUE)
```

use tidyr's separate() to split a column into multiple based on a delimiter. separate into two columns "word1" and "word2" before removing stop words 

```{r}
library(tidyr)

#split words into two columns to remove stop words 
tidy_bigrams_sep <- bigrams %>%
  separate(bigram, c("word1","word2"), sep= " ")

#filter stop words and non-alphabetical words 
tidy_bigrams_sep <- tidy_bigrams_sep %>%
  filter(!word1 %in% stop_words$word) %>%
  filter(!word2 %in% stop_words$word) %>%
  filter(!str_detect(word1, "[^[:alpha:]]")) %>%
  filter(!str_detect(word2, "[^[:alpha:]]")) %>%
  filter(!word1 %in% custom_stop_words$word) %>%
  filter(!word2 %in% custom_stop_words$word) %>%
  mutate(word1 = lemmatize_words(word1)) %>%
  mutate(word2 = lemmatize_words(word2))

#new bigram counts
bigram_counts <- tidy_bigrams_sep %>% 
  count(word1, word2, sort = TRUE)

View(bigram_counts)
```


```{r}
#recombine columns using unite() function 
tidy_bigrams_united <- tidy_bigrams_sep %>% 
  unite(bigram, word1, word2, sep= " ")

#count bigrams 
tidy_bigrams_united %>% count(bigram) %>% arrange(desc(n))
```


```{r}
#calculate bigram tf-idf
bigram_tf_idf <- tidy_bigrams_united %>%
  count(neighbourhood, bigram) %>%
  bind_tf_idf(bigram, neighbourhood, n) %>%
  arrange(desc(tf_idf))

head(bigram_tf_idf, n=10)

#subset top 5 tfidf neighbourhoods 
top5neigh_bigram <- subset(bigram_tf_idf, neighbourhood == 'Waterfront Communities-The Island'|neighbourhood == 'Niagara'| neighbourhood =='Annex'| neighbourhood == 'Bay Street Corridor'| neighbourhood == 'Church-Yonge Corridor')

head(top5neigh_bigram)

#plot top 5 neighbourhoods tf-idf 
top5neigh_bigram %>%
  arrange(desc(tf_idf)) %>%
  mutate(word = factor(bigram, levels = rev(unique(bigram)))) %>%
  group_by(neighbourhood) %>%
  top_n(15) %>%
  ungroup() %>%
  ggplot(aes(bigram, tf_idf, fill=neighbourhood)) + 
  geom_col(show.legend=FALSE) + 
  labs(x=NULL, y="tf-idf") + 
  facet_wrap(~neighbourhood, ncol=2, scales="free", shrink = TRUE) + 
  coord_flip()
```

Bigrams work a lot better in this instance because it takes away all names, and captures significant locations, and opionions


```{r, echo=FALSE}
#plot more frequent bigram in a bar graph
tidy_bigrams_united %>% 
  count(bigram) %>%
  arrange(desc(n)) %>%
  top_n(15) %>%
  ggplot(aes(bigram, n)) +
  geom_bar(stat="identity") + 
  coord_flip() + 
  xlab("Bigrams") + 
  ylab("Frequency") + 
  ggtitle("Most frequent bigrams")
```


```{r}
library(dplyr)
#wordcloud
tidy_bigrams_united %>%
  count(bigram) %>% 
  with(wordcloud(bigram, n, max.words=25, colors = brewer.pal(8,"Dark2")))
```

-----------

Use bigrams to provide context in sentiment analysis 

```{r}
tidy_bigrams_sep %>%
  filter(word1 == "terrible") %>%
  count(word1, word2, sort= TRUE)

#use AFINN lexicon 
AFINN <- get_sentiments("afinn")

#joining AFINN with tidy_bigrams_sep
terrible_words <- tidy_bigrams_sep %>%
  filter(word1 == "terrible") %>%
  inner_join(AFINN, by=c(word2="word")) %>%
  count(word2, value, sort=TRUE) 

head(terrible_words) 
```
Looking at the AFINN value for each word that has "bad" as the first word in the bigram. Because AFINN calculates single words, a word such as "luck" has a positive value, but when combined with bad to form "bad luck", it does not equate to a positive experience. This is why bigrams may prove to be more significant in reviewing comments. 

```{r}
#filter comments with "terrible"
mr %>%
  filter(str_detect(comments, "terrible experience")) %>%
  select(listing_id, neighbourhood, room_type, last_review, capped_price, calculated_host_listings_count, region, date, comments) -> terribleexperience
```


```{r}
#Comparison Plots 
library(ggplot2)

#bigrams that start with "bad" 
bad_words %>%
  mutate(contribution = n* value) %>%
  arrange(desc(abs(contribution))) %>%
  head(20) %>%
  mutate(word2 = reorder(word2, contribution)) %>%
  ggplot(aes(word2, n * value, fill = n * value > 0)) + 
  geom_col(show.legend=FALSE) + 
  xlab("Words preceded by \"bad\"") + 
  ylab("Sentiment value * number of occurences") + 
  coord_flip()
```

In viewing this comparison plot, as mentioned, bad luck is viewed as positive but in reality- it may not be positive. Some of the other words also need to be reviewed, as "bad woo" or "bad enjoy" are not two words that generally go together. 

On the negative end, "badass" is viewed as a positive experience in a social setting, as "bad bad" is not generally a word used. Again, more filters may need to be conducted in order to come up with realiable comparison graphs. 

##Trigrams 
```{r}
trigrams <- 
  mr %>% unnest_tokens(trigrams, comments, token = "ngrams", n=3)

#count and filter trigrams 
trigrams %>%
  count(trigrams, sort=TRUE)
```

```{r}
#split words into three columns to remove stop words 
tidy_trigrams_sep <- trigrams %>%
  separate(trigrams, c("word1","word2","word3"), sep= " ")

head(tidy_trigrams_sep)

#filter stop words and non-alphabetical words 
tidy_trigrams_sep <- tidy_trigrams_sep %>%
  filter(!word1 %in% stop_words$word) %>%
  filter(!word2 %in% stop_words$word) %>%
  filter(!word3 %in% stop_words$word) %>%
  filter(!str_detect(word1, "[^[:alpha:]]")) %>%
  filter(!str_detect(word2, "[^[:alpha:]]")) %>%
  filter(!str_detect(word3, "[^[:alpha:]]")) %>%
  filter(!word1 %in% custom_stop_words$word) %>%
  filter(!word2 %in% custom_stop_words$word) %>%
  filter(!word3 %in% custom_stop_words$word) %>%
  mutate(word1 = lemmatize_words(word1)) %>%
  mutate(word2 = lemmatize_words(word2)) %>%
  mutate(word3 = lemmatize_words(word3)) 


#new bigram counts
tri_counts <- tidy_trigrams_sep %>% 
  count(word1, word2, word3, sort = TRUE)

head(tri_counts)
```

```{r}
#recombine columns using unite() function 
tidy_trigrams_united <- tidy_trigrams_sep %>% 
  unite(trigrams, word1, word2, word3, sep= " ")

#count bigrams 
tidy_trigrams_united %>% count(trigrams) %>% arrange(desc(n))
```

```{r}
#calculate tri tf-idf by neighbourhood 
trigram_tf_idf <- tidy_trigrams_united %>%
  count(neighbourhood, trigrams) %>%
  bind_tf_idf(trigrams, neighbourhood, n) %>%
  arrange(desc(tf_idf))

head(trigram_tf_idf, n=10)

#subset top 5 tfidf neighbourhoods 
top5neigh_trigram <- subset(trigram_tf_idf, neighbourhood == 'Waterfront Communities-The Island'|neighbourhood == 'Niagara'| neighbourhood =='Annex'| neighbourhood == 'Bay Street Corridor'| neighbourhood == 'Church-Yonge Corridor')

head(top5neigh_trigram)

#plot top 5 neighbourhoods tf-idf 
top5neigh_trigram %>%
  arrange(desc(tf_idf)) %>%
  mutate(word = factor(trigrams, levels = rev(unique(trigrams)))) %>%
  group_by(neighbourhood) %>%
  top_n(15) %>%
  ungroup() %>%
  ggplot(aes(trigrams, tf_idf, fill=neighbourhood)) + 
  geom_col(show.legend=FALSE) + 
  labs(x=NULL, y="tf-idf") + 
  facet_wrap(~neighbourhood, ncol=2, scales="free", shrink = TRUE) + 
  coord_flip()
```

Highest TF-IDF's shown here include places of interest including air canada center, cn tower, rogers, center, etc. Subway stations also stand out in these neighbourhoods. Thus, transportation and places of interest are key for guests staying in Toronto Airbnb. 

One trigram that stands out is "pool hot tub". This is an addition to guests that can have a huge impact. 

```{r}
#calculate tri tf-idf by REGION
trigram_tf_idf_region <- tidy_trigrams_united %>%
  count(region, trigrams) %>%
  bind_tf_idf(trigrams, region, n) %>%
  arrange(desc(tf_idf))

head(trigram_tf_idf_region, n=10)

#subset top 5 tfidf neighbourhoods 
top5neigh_trigram <- subset(trigram_tf_idf, neighbourhood == 'Waterfront Communities-The Island'|neighbourhood == 'Niagara'| neighbourhood =='Annex'| neighbourhood == 'Bay Street Corridor'| neighbourhood == 'Church-Yonge Corridor')

head(top5neigh_trigram)

#plot tf-idf by region
trigram_tf_idf_region %>%
  arrange(desc(tf_idf)) %>%
  mutate(word = factor(trigrams, levels = rev(unique(trigrams)))) %>%
  group_by(region) %>%
  top_n(10) %>%
  ggplot(aes(trigrams, tf_idf, fill=region)) + 
  geom_col(show.legend=FALSE) + 
  labs(x=NULL, y="tf-idf") + 
  facet_wrap(~region, ncol=2, scales="free", shrink = TRUE) + 
  coord_flip()
```

When separated into regions, it becomes even more clear that public transportation/subways systems are crucial for a guest. In Etobicoke, being close to the airport sems to be the most important. 

#WORD SENTIMENT CATEGORIZATION 

In Tidytext - there are three general purpose lexicons (based on unigrams)
- Create custom list of stop words 
- Tokenize using tidytext 

##Afinn 

1.) AFINN: The AFINN lexicon assigns words with a score that runs between -5 and 5, with negative scores indicating negative sentiment and positive scores indicating positive sentiment.

```{r}
#run afinn by listing_id (with tokenized dataframe review_comments from exploring unigrams)
afinn <- review_comments %>%
  inner_join(get_sentiments("afinn"))

#group by listing, count sentiment (positive minus negative for total number)
afinnlevels <- afinn %>%
  group_by(listing_id) %>%
  summarise(afinnsentiment=sum(value))

#left join afinn to dfclean & replace NAs with 0 
dfclean %>% 
  left_join(afinnlevels) %>%
  replace_na(list(afinnsentiment=0)) -> dfclean

head(dfclean)
```

```{r}
#another way to summarise/split into positive and negative 
afinnsent <- afinn %>%
  group_by(listing_id) %>%
  mutate(polarity_level=ifelse(value >0, "Positive","Negative")) %>%
  count(polarity_level)
    
head(afinnsent)
```

Some of the scores are negative, neutral (including the outlier of -188) because of non-english characters (this needs work). Sentiment score on dfclean is the sum of all the values. 

##Bing 

2.)  Bing: The bing lexicon categorizes words in a binary fashion into positive and negative categories.

```{r}
#UNNEST_TOKENS & calculate bing lexicon sentiment
review_comments %>%
  inner_join(get_sentiments("bing")) -> bingreviews

#count sentiments in separate columns and calculate net sentiment (positive - negative)
bingsent <- bingreviews %>%
  group_by(listing_id) %>% #grouped by listing_id 
  count(neighbourhood, sentiment) %>%
  spread(sentiment, n, fill=0) %>%
  mutate(bingsentiment = positive - negative)

head(bingsent)

#subset just listing_id and sentiment score to join with dfclean
bing <- subset(bingsent, select=c(listing_id, bingsentiment))

#left join sentiments to review df & replace NAs with 0 
dfclean %>% 
  left_join(bing) %>%
  replace_na(list(bingsentiment=0)) -> dfclean
```

```{r}
#Comparison cloud of Bing sentiment using acast()
library(reshape2)

bingreviews %>%
  count(word, sentiment, sort= TRUE) %>%
  acast(word ~ sentiment, value.var="n", fill=0) %>%
  comparison.cloud(colors=c("red","blue"),
                   max.words = 100)
```


```{r}
#Filter negative reviews using Bing
bingnegative <- bingreviews %>%
  filter(sentiment == "negative")

head(bingnegative, n=20)
```

##NRC 

```{r}
#apply nrc sentiment to comments
review_comments %>%
  inner_join(get_sentiments("nrc") %>%
             filter(sentiment %in% c("positive","negative"))) -> nrcreviews

#group by listing ID and output sentiment score 
nrcsent <- nrcreviews %>%
  group_by(listing_id) %>%
  count(neighbourhood, sentiment) %>%
  spread(sentiment, n, fill=0) %>%
  mutate(nrcsentiment = positive-negative)

summary(nrcsent)
head(nrcsent)

#subset listing_id and nrc sentiment score 
nrc <- subset(nrcsent, select = c(listing_id, nrcsentiment))

#left join sentiments to review df & replace NAs with 0 
dfclean %>% 
  left_join(nrc) %>%
  replace_na(list(nrcsentiment=0)) -> dfclean

#View dfclean to confirm join 
head(dfclean)
```

##Using SentimentR 
- used to calculate text polarity sentiment at the sentence level (can aggregate by rows or grouping variables)

WHY sentimentR? 
- Because Sentiment Rtakes into account valence shifters (negators, amplifiers, deamplifiers, and adversative conjunctions). 

Polarity score is calculated by a combined and augmented version of Jocker's (2017) & Rinker's augmented Hu & Liu's (2004) dictionaries in the lexicon package 

###Calculate Sentiment R score by listing and join onto dfclean 
```{r}
library(sentimentr)
head(mr) #full review comments since sentimentR uses get_sentences to analyze sentence wise 

#aggregate by listing id & calculate sentimentR score 
mr %>% 
  get_sentences() %>%
  sentiment_by(by=c("listing_id")) -> sl

#five number summary of sentiment scores 
summary(sl$ave_sentiment)

#view head of sl dataframe
head(sl, n=10)

#density plot of sentimentR score
sl %>%
  ggplot() + 
  geom_density(aes(ave_sentiment))

#view split of polarity level for SentimentR scores 
sl %>%
  mutate(polarity_level = ifelse(ave_sentiment > 0, "Positive","Negative")) %>%
  count(polarity_level) 

#check for NAs
colSums(is.na(sl)|sl == '')
```

Both the histogram and density plots show a spike in 0, which means there are some that did not have a sentimentR score at all. 

```{r}
#subset sentiment score with listing id
sln <- subset(sl, select = c(listing_id, ave_sentiment))

#left join sl to df & replace NAs with 0 
dfclean %>% 
  left_join(sln) %>%
  replace_na(list(ave_sentiment=0)) -> dfclean
```


```{r}
#save CSV with sentiment scores to prevent running code again
#write.csv(dfclean, "dfclean_sentiment.csv", row.names = FALSE)
```





